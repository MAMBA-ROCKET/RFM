# -*- coding: utf-8 -*-

import numpy as np
import torch
import torch.nn as nn
import math
from scipy.linalg import lstsq,pinv
import matplotlib.pyplot as plt
import random
import torch.optim as optim
import matplotlib
matplotlib.use('TkAgg')
import sympy as sp
import scipy.optimize as sciopt
import copy
import time
from scipy.linalg import solve as scipy_solve
from scipy.optimize import minimize
import pandas as pd





eps = 1e-5
disturb = torch.zeros((200,1))
disturb[1,0] = eps



# fix random seed
torch.set_default_dtype(torch.float64)
def set_seed(x):
    random.seed(x)
    np.random.seed(x)
    torch.manual_seed(x)
    torch.cuda.manual_seed_all(x)
    torch.backends.cudnn.deterministic = True

set_seed(100)

# random initialization for parameters in FC layer
def weights_init(m):
    if isinstance(m, (nn.Conv2d, nn.Linear)):
        nn.init.uniform_(m.weight, a = -1, b = 1)
        nn.init.uniform_(m.bias, a = -1, b = 1)
        #nn.init.normal_(m.weight, mean=0, std=1)
        #nn.init.normal_(m.bias, mean=0, std=1)

# network definition
class RFM_rep(nn.Module):
    def __init__(self, in_features, J_n, x_max, x_min):
        super(RFM_rep, self).__init__()
        self.in_features = in_features
        self.hidden_features = J_n # hidden features as the number of basis functions in every point
        self.J_n = J_n
        self.x_min = x_min
        self.x_max = x_max
        self.a = 2.0/(x_max - x_min) # 1/radius
        self.x_0 = (x_max + x_min)/2 # center point
        self.hidden_layer = nn.Sequential(nn.Linear(self.in_features, self.hidden_features, bias=True),nn.Tanh()) 
        # nn.linear will generate Jn basis functions and they will pass into the next layer
        # this NN is learning the weight of every basis function I guess this makes sense

    # def forward(self,x):
    #     d = (x - self.x_min) / (self.x_max - self.x_min)
    #     #d = (x - self.x_0) * self.a
    #     d0 = (d <= -1/4)
    #     d1 = (d <= 1/4)  & (d > -1/4)
    #     d2 = (d <= 3/4)  & (d > 1/4)
    #     d3 = (d <= 5/4)  & (d > 3/4)
    #     d4 = (d > 5/4)
    #     y = self.a * (x - self.x_0)  # here y is the input of hidden layer, as the x variable
    #     y = self.hidden_layer(y) # The left hand side y is the local approximation of every point 
    #     y0 = 0
    #     y1 = y * (1 + torch.sin(2*np.pi*d) ) / 2
    #     y2 = y
    #     y3 = y * (1 - torch.sin(2*np.pi*d) ) / 2
    #     y4 = 0
    #     if self.x_min == 0:
    #         return(d0*y0+(d1+d2)*y2+d3*y3+d4*y4)
    #     elif self.x_max == interval_length:
    #         return(d0*y0+d1*y1+(d2+d3)*y2+d4*y4)
    #     else:
    #         return(d0*y0+d1*y1+d2*y2+d3*y3+d4*y4)
        

    def forward(self,x):
        d = (x - self.x_min) / (self.x_max - self.x_min)
        #d = (x - self.x_0) * self.a
        d0 = (d <= -1/4)
        d1 = (d <= 1/4)  & (d > -1/4)
        d2 = (d <= 3/4)  & (d > 1/4)
        d3 = (d <= 5/4)  & (d > 3/4)
        d4 = (d > 5/4)
        y = self.a * (x - self.x_0)  # here y is the input of hidden layer, as the x variable
        y = self.hidden_layer(y) # The left hand side y is the local approximation of every point 
        y0 = 0
        y1 = y * (1 + torch.sin(2*np.pi*d) ) / 2
        y2 = y
        y3 = y * (1 - torch.sin(2*np.pi*d) ) / 2
        y4 = 0
        if self.x_min == 0:
            return((d1+d2)*y2+d3*y3)
            #return(d0*y0+(d1+d2)*y2+d3*y3+d4*y4)
        elif self.x_max == interval_length:
            return(d1*y1+(d2+d3)*y2)
            #return(d0*y0+d1*y1+(d2+d3)*y2+d4*y4)
        else:
            return(d1*y1+d2*y2+d3*y3)
            #return(d0*y0+d1*y1+d2*y2+d3*y3+d4*y4)

class Connect(nn.Module):
    def __init__(self, J_n, M_p):
        super(Connect, self).__init__()
        self.input = J_n * M_p
        self.hidden_features = J_n # hidden features as the number of basis functions in every point
        self.hidden_layer = nn.Linear(self.input, 1, bias=False)


    def forward(self,x): # x is the input where x is either Lu or u

        y = self.hidden_layer(x) # y is the final solution of u or Lu

        return y

def cal_PDE(models,x,M_p,J_n,Q):
    Lu = np.array([])
        
    # forward and grad
    # for m in range(M_p):
    #     out = models[m](x)
    #     values = out.detach().numpy()
    #     grads = []
    #     grads_2 = []
    #     for i in range(J_n):
    #         g_1 = torch.autograd.grad(outputs=out[:,i], inputs=x,
    #                                 grad_outputs=torch.ones_like(out[:,i]),
    #                                 create_graph = True, retain_graph = True)[0]
    #         grads.append(g_1.squeeze().detach().numpy())
    #         g_2 = torch.autograd.grad(outputs=g_1[:,0], inputs=x,
    #                                 grad_outputs=torch.ones_like(out[:,i]),
    #                                 create_graph = False, retain_graph = True)[0]
    #         grads_2.append(g_2.squeeze().detach().numpy())
    #     grads = np.array(grads).T
    #     grads_2 = np.array(grads_2).T
    #     if Lu.size == 0:
    #         Lu = grads_2
    #     else:
    #         Lu = np.hstack((Lu,grads_2))
        
    for m in range(M_p):
        out = models[m](x)
        values = out.detach().numpy()
        #u = torch.cat((u,values),dim=0)
        if Lu.size == 0:
            Lu = values
        else:
            Lu = np.hstack((Lu,values))

    

    return Lu  # as the input layer of PDE connection

def cal_boundary(models,x,M_p,J_n,Q):
    u = np.array([])

    # forward and grad
    for m in range(M_p):
        out = models[m](x)
        values = out.detach().numpy()
        #u = torch.cat((u,values),dim=0)
        if u.size == 0:
            u = values
        else:
            u = np.hstack((u,values))

    return u  # as the input layer of boundary connection


def scaling(Lu,u):

    lambda_i = torch.max(torch.abs(Lu),dim=1)
    lambda_o = torch.max(torch.abs(u),dim=1)

    return lambda_i,lambda_o
        
def compute_loss(x_interior_col,x_boundary_col,models,net,M_p,J_n,Q,Lu,u,lambda_i,lambda_o):
    #Lu = cal_PDE(models,x_interior_col,M_p,J_n,Q)
    Lu = torch.tensor(Lu,requires_grad=False)
    Lu = Lu.double()
    #print('haha',1)
    #u = cal_boundary(models,x_boundary_col,M_p,J_n,Q)
    u = torch.tensor(u,requires_grad=False)
    u = u.double()
    input = torch.cat((Lu,u),dim=0)
    output_PDE = net(Lu)
    output_boundary = net(u)
    output = torch.cat((output_PDE,output_boundary),dim=0)
    target_PDE = Lu_f(x_interior_col).view(-1,1)
    target_PDE = target_PDE.double()
    target_boundary = anal_u(x_boundary_col)
    target_boundary = target_boundary.double()
    target = torch.cat((target_PDE,target_boundary),dim=0)
    # loss_f = torch.sum((target_PDE)**2) + torch.sum((target_boundary)**2)
    # loss = torch.sum((output_PDE - target_PDE)**2) + torch.sum((output_boundary - target_boundary)**2)
    loss = torch.sum((output - target)**2)
    return loss

# analytical solution parameters
AA = 1
aa = 2.0*torch.pi
bb = 3.0*torch.pi
interval_length = 8.
lamb = 4

def anal_u(x):
    return AA * torch.sin(bb * (x + 0.05)) * torch.cos(aa * (x + 0.05)) + 2.0

def anal_dudx_1st(x):
    return AA * (bb*torch.cos(bb*(x+0.05))*torch.cos(aa*(x+0.05)) - aa*torch.sin(bb*(x+0.05))*torch.sin(aa*(x+0.05)))

def anal_dudx_2nd(x):
    return -AA*(aa*aa+bb*bb)*torch.sin(bb*(x+0.05))*torch.cos(aa*(x+0.05))\
           -2.0*AA*aa*bb*torch.cos(bb*(x+0.05))*torch.sin(aa*(x+0.05))

def Lu_f(pointss, lambda_ = 4):
    r = torch.tensor([],requires_grad=False)
    for x in pointss:
        # f = anal_dudx_2nd(x)
        f = anal_u(x)
        r = torch.cat((r,f),dim=0)
    return(r)

# define the local-networks and points in the corresponding regions
def pre_define(M_p,J_n,Q):
    models = []
    points = []
    for k in range(M_p):
        x_min = 8.0/M_p * k
        x_max = 8.0/M_p * (k+1) # This means chopping the interval into M_p pieces and deal with each piece seperately
        model = RFM_rep(in_features = 1, J_n = J_n, x_min = x_min, x_max = x_max) # this in_feature is the input of the first layer, as x?
        model = model.apply(weights_init)
        model = model.double()
        for param in model.parameters():
            param.requires_grad = False
        models.append(model)
        points.append(torch.tensor(np.linspace(x_min, x_max, Q+1),requires_grad=True).reshape([-1,1]))
        # Every interval has Q+1 points, and the first and last point are the boundary points
    #print('points:',points)
    #print('number of points:',len(points))
    return(models,points)

def closure():
    optimizer.zero_grad()
    loss = compute_loss(points,points_boundary,models,net,M_p,J_n,Q,Lu,u,lambda_i,lambda_o)
    #loss.backward(retain_graph=True)
    loss.backward()
    return loss.item()


class GaussNewtonOptimizer:
    def __init__(self, model, lr):
        self.model = model
        self.lr = lr

    def step(self, input, target):
        outputs = self.model(input)
        residuals = outputs - target
        # residuals = residuals.view(-1)
        jacobian = torch.zeros((len(residuals), 200))
        for i, r in enumerate(residuals):
            grad_params = torch.autograd.grad(r, self.model.parameters(), retain_graph=True, allow_unused=True, create_graph=False)
            temp = grad_params[0].view(-1)
            # jacobian[i,:] = torch.cat([g.view(-1) for g in grad_params])
            jacobian[i,:] = temp
        # jacobian = torch.autograd.functional.jacobian(self.model, input, create_graph=True)
        # jacobian = torch.autograd.functional.jacobian(residuals, self.model.parameters(), create_graph=True)
        g_n_update = torch.pinverse(jacobian.T @ jacobian) @ jacobian.T @ residuals

        with torch.no_grad():
            for param in self.model.parameters():
                param -= self.lr * g_n_update.T


M_p = 4
J_n = 50
Q = 50

models,points = pre_define(M_p,J_n,Q)

points = torch.cat(points,dim=0)
points1 = torch.linspace(0.,8.,201)[1:-1]
points = points.view(-1,1)
#points.requires_grad = True
points_boundary = torch.tensor([0.,8.]).view(-1,1)



net = Connect(J_n,M_p)
net.double()

#optimizer = GradientDescentLineSearch(net.parameters())
#optimizer = optim.Adam(net.parameters(), lr=0.1)
# optimizer = optim.LBFGS(net.parameters(),max_iter=20,tolerance_grad=1e-15, tolerance_change=1e-16,lr=0.1,history_size=200)#line_search_fn='strong_wolfe',
optimizer = GaussNewtonOptimizer(net,1)
# torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None)

# net.hidden_layer.weight.data = torch.zeros_like(net.hidden_layer.weight.data)


# params = list(net.parameters())

# for p in params:
#     if p.grad is not None:
#         p.grad.zero_()

# for p in params:
#     if p.grad is not None:
#         print('current:',p.grad)


# loss = compute_loss(points, points_boundary,models,net,M_p,J_n,Q)
# # x_interior_col.requires_grad = False
# # x_boundary_col.requires_grad = False
# loss.backward(create_graph=True,retain_graph=True)

# for p in params:
#     if p.grad is not None:
#         print('new:',p.grad)


# grad_params = torch.cat([p.grad.view(-1) for p in params if p.grad is not None])

# hessian = torch.zeros((len(grad_params), len(grad_params)))
# for i, g in enumerate(grad_params):
#     print(i)
#     hvp = torch.autograd.grad(g, params, retain_graph=True, allow_unused=True, create_graph=False)
#     hvp_cat = torch.cat([h.view(-1) for h in hvp])
#     hessian[i] = hvp_cat


# #print('the first element:',hessian[1,1])
# print('hessian:',hessian)
# print('grad_params:',grad_params)



# numpy_matrix = hessian.numpy()
# non_zero_count = np.count_nonzero(numpy_matrix)
# print('check:',non_zero_count)
# numpy_vector = grad_params.detach().numpy()

# #result = np.linalg.solve(numpy_matrix, -numpy_vector)
# result = lstsq(numpy_matrix, -numpy_vector)[0]
# residue = lstsq(numpy_matrix, -numpy_vector)[1]
# rank = lstsq(numpy_matrix, -numpy_vector)[2]


# Error = np.dot(numpy_matrix,result) + numpy_vector

# numpy_vector = numpy_vector + result
Lu = cal_PDE(models,points,M_p,J_n,Q)
df = pd.DataFrame(Lu)
df.to_excel("Lu_mine.xlsx", index=False, header=False, engine='openpyxl')
Lu = torch.tensor(Lu,requires_grad=False)
u = cal_boundary(models,points_boundary,M_p,J_n,Q)
df = pd.DataFrame(u)
df.to_excel("u_mine.xlsx", index=False, header=False, engine='openpyxl')
u = torch.tensor(u,requires_grad=False)

lambda_i,lambda_o = scaling(Lu,u)
lambda_i = lambda_i[0].view(-1,1)
print(max(lambda_i))
lambda_o = lambda_o[0].view(-1,1)
print(max(lambda_o))
points = torch.tensor(points,requires_grad=False)


Lu = torch.tensor(Lu,requires_grad=False)
Lu = Lu.double()
u = torch.tensor(u,requires_grad=False)
u = u.double()
input = torch.cat((Lu,u),dim=0)
target_PDE = Lu_f(points).view(-1,1)
target_PDE = target_PDE.double()
target_boundary = anal_u(points_boundary)
target_boundary = target_boundary.double()
target = torch.cat((target_PDE,target_boundary),dim=0)



flag = 1
epoch = 0
# Training
while flag == 1:
    print('epoch:',epoch)



    total_loss = compute_loss(points,points_boundary,models,net,M_p,J_n,Q,Lu,u,lambda_i,lambda_o)
    # This is for Gauss-Newton
    optimizer.step(input, target)
    loss = torch.nn.functional.mse_loss(net(input), target)




    # This is for Adam
    # total_loss = compute_loss(points, points_boundary,models,net,M_p,J_n,Q,Lu,u)
    # optimizer.zero_grad()
    # total_loss.backward()
    # optimizer.step()


    # This is for line search method
    # total_loss = compute_loss(points, points_boundary,models,net,M_p,J_n,Q,Lu,u)
    # optimizer.zero_grad()
    # total_loss.backward(retain_graph=True)
    # optimizer.step(compute_loss)


    # This is for LBFGS method
    # print('start')
    # start_time = time.time()
    # optimizer.step(closure)
    # end_time = time.time()
    # print('end')
    # print('time:',end_time-start_time)

    # net.hidden_layer.weight.data = torch.ones_like(net.hidden_layer.weight.data)
    # for i in range(len(numpy_vector)):
    #     net.hidden_layer.weight.data[0,i] = numpy_vector[i]



    # total_loss = compute_loss(points,points_boundary,models,net,M_p,J_n,Q,Lu,u,lambda_i,lambda_o)
    print('total_loss:',total_loss)

    # for param in net.parameters():
    #     print('grad:',torch.max(torch.abs(param.grad)))


    # This is for Adam
    # total_loss = compute_loss(x_interior_col, x_boundary_col, u_global_test, co_global_test)
    # optimizer.zero_grad()
    # total_loss.backward()
    # optimizer.step()
    

    epoch += 1

    if epoch % 1 == 0:
        u_plot = cal_boundary(models,points,M_p,J_n,Q)
        u_plot = torch.tensor(u_plot,requires_grad=False)

        U = net(u_plot)
        U = U.detach().numpy()
        points_plot = points.detach().numpy()
        U_true = anal_u(points)
        U_true = U_true.detach().numpy()
        Error = np.abs(U - U_true)
        Errormax = np.max(Error)
        plt.figure(int(epoch))
        plt.plot(points_plot,U)
        plt.title('Approximate Solution')
        plt.xlabel('x')
        plt.ylabel('y')  

        plt.draw()  # Update the figure
        plt.pause(5)  # Wait for 0.1 seconds
        plt.close()  # Clear the figure for the next plot
        
        
for name, param in net.named_parameters():
    print(name, param.grad)




# calculate the matrix A,f in linear equations system 'Au=f'
def cal_matrix(models,points,M_p,J_n,Q):
    # matrix define (Aw=b)
    A_1 = np.zeros([M_p*Q,M_p*J_n])
    A_2 = np.zeros([2,M_p*J_n])
    f = np.zeros([M_p*Q + 2, 1])
    
    for k in range(M_p):
        # forward and grad
        for m in range(M_p):
            out = models[m](points[k])
            values = out.detach().numpy()
            grads = []
            grads_2 = []
            for i in range(J_n):
                g_1 = torch.autograd.grad(outputs=out[:,i], inputs=points[k],
                                      grad_outputs=torch.ones_like(out[:,i]),
                                      create_graph = True, retain_graph = True)[0]
                grads.append(g_1.squeeze().detach().numpy())
                g_2 = torch.autograd.grad(outputs=g_1[:,0], inputs=points[k],
                                      grad_outputs=torch.ones_like(out[:,i]),
                                      create_graph = False, retain_graph = True)[0]
                grads_2.append(g_2.squeeze().detach().numpy())
            grads = np.array(grads).T
            grads_2 = np.array(grads_2).T
            Lu = grads_2
            print(Lu.shape)
            # Lu = f condition
            A_1[k*Q:(k + 1)*Q, m*J_n:(m + 1)*J_n] = Lu[:Q,:]
            # boundary condition
            if k == 0 and m==k:
                A_2[0, :J_n] = values[0,:]
            elif k == M_p - 1 and m==k:
                A_2[1, -J_n:] = values[-1,:]
                
        true_f = Lu_f(points[k].detach().numpy(), lamb).reshape([(Q + 1),1])
        f[k*Q:(k + 1)*Q,: ] = true_f[:Q]
    A = np.concatenate((A_1,A_2),axis=0)
    f[M_p*Q,:] = anal_u(0.)
    f[M_p*Q+1,:] = anal_u(8.)
    return(A,f)

# calculate the l^{inf}-norm and l^{2}-norm error for u,v,p
def test(models,M_p,J_n,Q,w,plot = True):
    epsilon = []
    true_values = []
    numerical_values = []
    test_Q = int(1000/M_p)
    for k in range(M_p): # k is for domain decomposition
        points = torch.tensor(np.linspace(8.0/M_p * (k), 8.0/M_p * (k+1), test_Q+1),requires_grad=False).reshape([-1,1])
        #print('points:',points)
        out_total = None
        for m in range(M_p):
            out = models[m](points) # m is for model defined in every subdomain
            values = out.detach().numpy()
            if out_total is None:
                out_total = values
            else:
                out_total = np.concatenate((out_total,values),axis=1)
        true_value = anal_u(points.numpy()).reshape([-1,1])
        #print('out_total:',out_total)
        numerical_value = np.dot(np.array(out_total), w)
        #print('numerical_value:',numerical_value)
        true_values.extend(true_value)
        numerical_values.extend(numerical_value)
        epsilon.extend(true_value - numerical_value)
    true_values = np.array(true_values)

    numerical_values = np.array(numerical_values)
    # for i in range(len(numerical_values)):
    #     print(numerical_values[i])
    #print('Approximate solution:',numerical_values)
    #print('Exact solution:',true_values)
    # print(true_values.shape)
    print(np.max(np.abs(true_values - numerical_values)))
    epsilon = np.array(epsilon)
    epsilon = np.maximum(epsilon, -epsilon)
    print('********************* ERROR *********************')
    print('M_p=%s,J_n=%s,Q=%s'%(M_p,J_n,Q))
    print('L_inf=',epsilon.max(),'L_2=',math.sqrt(8*sum(epsilon*epsilon)/len(epsilon)))
        
    x = [(interval_length/M_p)*i / test_Q  for i in range(M_p*(test_Q+1))]
    if plot == True:
        plt.figure()
        plt.plot(x, true_values, label = "exact solution", color='black')
        plt.plot(x, numerical_values, label = "numerical solution", color='darkblue', linestyle='--')
        plt.legend()
        plt.title('exact solution')
        plt.savefig('./numerical_solution.pdf', dpi=100)
        
        plt.figure()
        plt.plot(x, epsilon, label = "absolute error", color='black')
        plt.legend()
        plt.title('RFM error, $\psi^2$, J_n=%s Q=%s'%(M_p*J_n,M_p*Q))
        #plt.savefig('./error_Ne=%sJ_n=%sQ=%s.pdf'%(M_p,J_n,Q), dpi=100)
    return(epsilon.max(),math.sqrt(8*sum(epsilon*epsilon)/len(epsilon)))

def main(M_p,J_n,Q,plot = True, moore = False):
    # prepare models and collocation pointss
    models,points = pre_define(M_p,J_n,Q)
    
    # matrix define (Aw=b)
    A,f = cal_matrix(models,points,M_p,J_n,Q)

    hessian = np.matmul(A.T,A)
    print('hessian',hessian)

    print('loss:',np.sum(f**2))
    
    # solve
    if moore:
        inv_coeff_mat = pinv(A)  # moore-penrose inverse, shape: (n_units,n_colloc+2)
        w = np.matmul(inv_coeff_mat, f)
    else:
        w = lstsq(A,f)[0]
        res = (np.dot(A,w) - f)
        print('residue:',np.max(np.abs(res)))
        #print('residue:',lstsq(A,f)[1])
    
    # test
    w = np.ones_like(w)
    # for i,res in enumerate(w):
    #     print(res)
    #     print(i)
    print('number of coefficient:',len(w))
    return(test(models,M_p,J_n,Q,w,plot))

if __name__ == '__main__':
    set_seed(100)
    #M_p = 4 # the number of basis center points
    J_n = 50 # the number of basis functions per center points
    Q = 50 # the number of collocation pointss per basis functions support
    for M_p in [4]:
        main(M_p,J_n,Q,True,False)